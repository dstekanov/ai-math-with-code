{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---- BatchNorm \"live\" step-by-step demo ----\n",
        "# We simulate a Dense layer's pre-activations z for a batch of 4 samples and 3 units.\n",
        "z = np.array([\n",
        "    [1.0,  2.0, 3.0],\n",
        "    [2.0,  0.0, 4.0],\n",
        "    [0.0, -1.0, 2.0],\n",
        "    [1.0,  1.0, 5.0],\n",
        "], dtype=np.float32)\n",
        "\n",
        "epsilon = 1e-5  # numerical stability, typical Keras default around 1e-3..1e-5\n",
        "gamma   = np.array([1.0, 0.5, 2.0], dtype=np.float32)  # learnable scale per feature\n",
        "beta    = np.array([0.0, 1.0, -1.0], dtype=np.float32) # learnable shift per feature\n",
        "\n",
        "print(\"Pre-activation z (shape batch=4, features=3):\\n\", z, \"\\n\")\n",
        "\n",
        "# 1) Batch statistics (per feature/column)\n",
        "mu_B  = z.mean(axis=0)                   # batch mean\n",
        "var_B = z.var(axis=0)                    # batch variance (population, ddof=0)\n",
        "\n",
        "print(\"Batch mean (mu_B):\", mu_B)\n",
        "print(\"Batch variance (var_B):\", var_B, \"\\n\")\n",
        "\n",
        "# 2) Normalize\n",
        "z_hat = (z - mu_B) / np.sqrt(var_B + epsilon)\n",
        "\n",
        "# 3) Affine transform with learnable parameters\n",
        "y = gamma * z_hat + beta\n",
        "\n",
        "print(\"Normalized z (z_hat):\\n\", np.round(z_hat, 4), \"\\n\")\n",
        "print(\"Mean(z_hat) per feature (≈0):\", np.round(z_hat.mean(axis=0), 6))\n",
        "print(\"Var(z_hat)  per feature (≈1):\", np.round(z_hat.var(axis=0), 6), \"\\n\")\n",
        "\n",
        "print(\"gamma:\", gamma, \"beta:\", beta)\n",
        "print(\"BatchNorm output y = gamma*z_hat + beta:\\n\", np.round(y, 4), \"\\n\")\n",
        "\n",
        "# 4) (Optional) What happens if the previous layer shifts/scales z?\n",
        "z_shifted = 2.0 * z + 10.0  # strong scale and shift\n",
        "mu_B_s  = z_shifted.mean(axis=0)\n",
        "var_B_s = z_shifted.var(axis=0)\n",
        "z_hat_s = (z_shifted - mu_B_s) / np.sqrt(var_B_s + epsilon)\n",
        "y_s     = gamma * z_hat_s + beta\n",
        "\n",
        "print(\"After strong shift/scale (z' = 2*z + 10):\")\n",
        "print(\"mu_B':\", mu_B_s, \"var_B':\", var_B_s)\n",
        "print(\"Normalized z' (z_hat'):\\n\", np.round(z_hat_s, 4))\n",
        "print(\"Mean(z_hat') per feature (≈0):\", np.round(z_hat_s.mean(axis=0), 6))\n",
        "print(\"Var(z_hat')  per feature (≈1):\", np.round(z_hat_s.var(axis=0), 6))\n",
        "print(\"Output y' with same gamma,beta:\\n\", np.round(y_s, 4), \"\\n\")\n",
        "\n",
        "# 5) (Optional) One-step running stats update (as in inference time usage)\n",
        "momentum = 0.9\n",
        "running_mean = np.zeros_like(mu_B)\n",
        "running_var  = np.ones_like(var_B)\n",
        "\n",
        "running_mean = momentum * running_mean + (1.0 - momentum) * mu_B\n",
        "running_var  = momentum * running_var  + (1.0 - momentum) * var_B\n",
        "\n",
        "print(\"One-step running stats update (momentum=0.9):\")\n",
        "print(\"running_mean:\", np.round(running_mean, 6))\n",
        "print(\"running_var :\", np.round(running_var, 6))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
